{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quora pairs\n",
    "### Issue\n",
    "- Many people ask similarly worded questions\n",
    "- Multiple questions with the same intent can cause seekers to spend more time finding the best answer to their question\n",
    "- Make writers feel they need to answer multiple versions of the same question. \n",
    "\n",
    "Quora values canonical questions because they provide a better experience to active seekers and writers, and offer more value to both of these groups in the long term.\n",
    "\n",
    "### Current technique\n",
    "Quora uses a Random Forest model to identify duplicate questions\n",
    "\n",
    "### Goal\n",
    "Classify whether question pairs are duplicates or not. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from time import time\n",
    "\n",
    "import sys\n",
    "stdout = sys.stdout\n",
    "reload(sys)\n",
    "sys.setdefaultencoding('utf-8')\n",
    "sys.stdout = stdout\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data exploration\n",
    "According to the description: \n",
    "- __id__: the id of a training set question pair\n",
    "- __qid1__, __qid2__: unique ids of each question (only available in train.csv)\n",
    "- __question1__, __question2__: the full text of each question\n",
    "- __is_duplicate__: the target variable, set to 1 if question1 and question2 have essentially the same meaning, and 0 otherwise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>qid1</th>\n",
       "      <th>qid2</th>\n",
       "      <th>question1</th>\n",
       "      <th>question2</th>\n",
       "      <th>is_duplicate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>What is the step by step guide to invest in sh...</td>\n",
       "      <td>What is the step by step guide to invest in sh...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>What is the story of Kohinoor (Koh-i-Noor) Dia...</td>\n",
       "      <td>What would happen if the Indian government sto...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>How can I increase the speed of my internet co...</td>\n",
       "      <td>How can Internet speed be increased by hacking...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>Why am I mentally very lonely? How can I solve...</td>\n",
       "      <td>Find the remainder when [math]23^{24}[/math] i...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "      <td>10</td>\n",
       "      <td>Which one dissolve in water quikly sugar, salt...</td>\n",
       "      <td>Which fish would survive in salt water?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  qid1  qid2                                          question1  \\\n",
       "0   0     1     2  What is the step by step guide to invest in sh...   \n",
       "1   1     3     4  What is the story of Kohinoor (Koh-i-Noor) Dia...   \n",
       "2   2     5     6  How can I increase the speed of my internet co...   \n",
       "3   3     7     8  Why am I mentally very lonely? How can I solve...   \n",
       "4   4     9    10  Which one dissolve in water quikly sugar, salt...   \n",
       "\n",
       "                                           question2  is_duplicate  \n",
       "0  What is the step by step guide to invest in sh...             0  \n",
       "1  What would happen if the Indian government sto...             0  \n",
       "2  How can Internet speed be increased by hacking...             0  \n",
       "3  Find the remainder when [math]23^{24}[/math] i...             0  \n",
       "4            Which fish would survive in salt water?             0  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train = pd.read_csv('../data/train.csv').fillna(\"\")\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 404290 entries, 0 to 404289\n",
      "Data columns (total 6 columns):\n",
      "id              404290 non-null int64\n",
      "qid1            404290 non-null int64\n",
      "qid2            404290 non-null int64\n",
      "question1       404290 non-null object\n",
      "question2       404290 non-null object\n",
      "is_duplicate    404290 non-null int64\n",
      "dtypes: int64(4), object(2)\n",
      "memory usage: 18.5+ MB\n"
     ]
    }
   ],
   "source": [
    "df_train.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions\n",
    "- Are all the questions unique?\n",
    "- What is the ratio duplicate / non-duplicate pair questions?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique id questions: 66.53%\n",
      "Duplicate pair questions: 36.92%, non-duplicate questions 63.08%\n"
     ]
    }
   ],
   "source": [
    "unique_questions = df_train['qid1'].tolist() + df_train['qid2'].tolist() \n",
    "total = len(unique_questions)\n",
    "print \"Unique id questions: %.2f%%\" % (float(len(set(unique_questions)))/total*100) \n",
    "print \"Duplicate pair questions: %.2f%%, non-duplicate questions %.2f%%\" % (df_train['is_duplicate'].mean()*100, \n",
    "                                                                        100 - df_train['is_duplicate'].mean()*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A peek at the Test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_test = pd.read_csv('../data/test.csv')\n",
    "df_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning of the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os.path\n",
    "import re\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "    \n",
    "# How to clean?\n",
    "def clean_question(sentence, stop_words):\n",
    "    # Remove punctuation\n",
    "    sentence = ''.join([c for c in sentence if c not in string.punctuation])\n",
    "    # Lower and tokenization\n",
    "    tokenized_sentence = sentence.lower().split()\n",
    "    tokenized_sentence = [c for c in tokenized_sentence if c not in stop_words]\n",
    "    return ' '.join(tokenized_sentence)    \n",
    "    \n",
    "if not os.path.isfile('../data/cleaned_train.csv'): \n",
    "    df_train['clean_q1'] = df_train.apply(lambda x: clean_question(x['question1'], stop_words), axis=1)\n",
    "    df_train['clean_q2'] = df_train.apply(lambda x: clean_question(x['question2'], stop_words), axis=1)\n",
    "    df_train.to_csv('../data/cleaned_train.csv', index=False)\n",
    "else:\n",
    "    df_train = pd.read_csv('../data/cleaned_train.csv').fillna(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Features "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For features, I chose to work with the following features:\n",
    "- __Distance Measure__:\n",
    "    - __Jaccard Similarity__: ratio of number of shared terms against total number of terms.\n",
    "- __Shared__: \n",
    "    - __words (unigrams)__\n",
    "    - __bigrams__\n",
    "    - __trigrams__\n",
    "- __NLP-specific__: \n",
    "    - __Word movers distance__: semantic similarity between the questions\n",
    "    - __TD-IDF__: similarity between 2 strings from a corpus (google news here)\n",
    "- __Other__:\n",
    "    - __Same final words__: Last words might carry weights\n",
    "    - __Length of questions__: similar questions would have similar length\n",
    "    - __Ratio of length between questions__: normalized measure for the length\n",
    "    - __Question frequency__: a question asked often will be likely to be a duplicate\n",
    "\n",
    "Everything will be processed without stop words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['My', 'name', 'is', 'jean', 'luc?'] ['My_name', 'is_jean', 'luc?'] ['My_name_is', 'jean_paul.']\n"
     ]
    }
   ],
   "source": [
    "def ngram(l, n, joint=''):\n",
    "    return [joint.join(l[i:i + n]) for i in xrange(0, len(l), n)]\n",
    "       \n",
    "def unigram(sentence):\n",
    "    sentence = sentence.split()\n",
    "    return ngram(sentence,1)\n",
    "\n",
    "def bigram(sentence, joint=\"_\"):\n",
    "    sentence = sentence.split()\n",
    "    return ngram(sentence,2,joint)\n",
    "     \n",
    "def trigram(sentence, joint=\"_\"):\n",
    "    sentence = sentence.split()\n",
    "    return ngram(sentence,3,joint)\n",
    "\n",
    "q1 = \"My name is jean luc?\"\n",
    "q2 = \"My name is jean paul.\"\n",
    "\n",
    "print unigram(q1), bigram(q1), trigram(q2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A. Distance Measure\n",
    "#### Jaccard similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "\n",
    "def jaccard(q1, q2, debug=False):\n",
    "    q1_s = q1.split()\n",
    "    q2_s = q2.split()\n",
    "    shared_words = [word for word in set(q1_s).intersection(q2_s)]\n",
    "    shared_total = [word for word in set(q1_s).union(q2_s)]\n",
    "    if debug:\n",
    "        print shared_words, shared_total\n",
    "    if len(shared_total) == 0:\n",
    "        shared_total = [1]\n",
    "    return (len(shared_words) / len(shared_total))\n",
    "\n",
    "\n",
    "# jaccard(q1,q2, True)\n",
    "df_train['jaccard_sim'] = df_train.apply(lambda x: jaccard(x['clean_q1'], x['clean_q2']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0.833333\n",
       "1    0.222222\n",
       "2    0.222222\n",
       "3    0.000000\n",
       "4    0.153846\n",
       "Name: jaccard_sim, dtype: float64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train['jaccard_sim'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B. Shared words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def shared_unigrams(q1,q2):\n",
    "    return len(set(unigram(q1+q2)))\n",
    "\n",
    "def shared_bigrams(q1,q2):\n",
    "    return len(set(bigram(q1)).intersection(bigram(q2)))\n",
    "\n",
    "def shared_trigrams(q1,q2):\n",
    "    return len(set(trigram(q1)).intersection(trigram(q2)))\n",
    "\n",
    "# print shared_unigrams(q1,q2), shared_bigrams(q1,q2), shared_trigrams(q1,q2)\n",
    "\n",
    "df_train['shared_uni'] = df_train.apply(lambda x: shared_unigrams(x['clean_q1'], x['clean_q2']), axis=1)\n",
    "df_train['shared_bi'] = df_train.apply(lambda x: shared_bigrams(x['clean_q1'], x['clean_q2']), axis=1)\n",
    "df_train['shared_tri'] = df_train.apply(lambda x: shared_trigrams(x['clean_q1'], x['clean_q2']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0     6\n",
      "1     9\n",
      "2     9\n",
      "3     7\n",
      "4    12\n",
      "Name: shared_uni, dtype: int64 0    3\n",
      "1    1\n",
      "2    0\n",
      "3    0\n",
      "4    0\n",
      "Name: shared_bi, dtype: int64 0    2\n",
      "1    0\n",
      "2    0\n",
      "3    0\n",
      "4    0\n",
      "Name: shared_tri, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print df_train['shared_uni'].head(), df_train['shared_bi'].head(), df_train['shared_tri'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### C. NLP-specific"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cell took 172.59 seconds to run.\n"
     ]
    }
   ],
   "source": [
    "start = time()\n",
    "\n",
    "try:\n",
    "    from pyemd import emd\n",
    "    PYEMD_EXT = True\n",
    "except ImportError:\n",
    "    PYEMD_EXT = False\n",
    "    \n",
    "import gensim\n",
    "word2vec_model = gensim.models.KeyedVectors.load_word2vec_format(\n",
    "    '../data/GoogleNews-vectors-negative300.bin.gz', \n",
    "    binary=True)\n",
    "\n",
    "# Word movers distance\n",
    "def wmd(q1,q2,model):\n",
    "    return model.wmdistance(q1, q2)\n",
    "\n",
    "print 'Cell took %.2f seconds to run.' %(time() - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cell took 1175.54 seconds to run.\n"
     ]
    }
   ],
   "source": [
    "start = time()\n",
    "\n",
    "word2vec_model.init_sims(replace=True) \n",
    "# wmd(q1,q2,word2vec_model)\n",
    "df_train['norm_wmd'] = df_train.apply(lambda x: wmd(x['clean_q1'], x['clean_q2'],word2vec_model), axis=1)\n",
    "\n",
    "print 'Cell took %.2f seconds to run.' %(time() - start)\n",
    "df_train['norm_wmd'].head()\n",
    "df_train.to_csv('../data/cleaned_train_with_ABC_features.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TD-IDF\n",
    "from collections import Counter\n",
    "\n",
    "# If a word appears only once, we ignore it completely (likely a typo)\n",
    "# Epsilon defines a smoothing constant, which makes the effect of extremely rare words smaller\n",
    "def get_weight(count, eps=10000, min_count=2):\n",
    "    if count < min_count:\n",
    "        return 0\n",
    "    else:\n",
    "        return 1 / (count + eps)\n",
    "\n",
    "eps = 5000\n",
    "questions_serie = pd.Series(df_train['question1'].tolist() + df_train['question2'].tolist()).astype(str)\n",
    "\n",
    "words = (\" \".join(questions_serie)).lower().split()\n",
    "counts = Counter(words)\n",
    "weights = {word: get_weight(count) for word, count in counts.items()}\n",
    "\n",
    "def tfidf(q1,q2,stop_words):\n",
    "    q1words = {}\n",
    "    q2words = {}\n",
    "    for word in str(q1).lower().split():\n",
    "        if word not in stop_words:\n",
    "            q1words[word] = 1\n",
    "    for word in str(q2).lower().split():\n",
    "        if word not in stop_words:\n",
    "            q2words[word] = 1\n",
    "    if len(q1words) == 0 or len(q2words) == 0:\n",
    "        # The computer-generated chaff includes a few questions that are nothing but stopwords\n",
    "        return 0\n",
    "    \n",
    "    shared_weights = [weights.get(w, 0) for w in q1words.keys() if w in q2words] + [weights.get(w, 0) for w in q2words.keys() if w in q1words]\n",
    "    total_weights = [weights.get(w, 0) for w in q1words] + [weights.get(w, 0) for w in q2words]\n",
    "    \n",
    "    return np.sum(shared_weights, dtype=np.float64) / np.sum(total_weights, dtype=np.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_train['tdidf'] = df_train.apply(lambda x: tfidf(x['question1'],x['question2'],stop_words), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### D. Other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Same final words\n",
    "def final_words(q1,q2):\n",
    "    q1_s = q1.split()\n",
    "    q2_s = q2.split()\n",
    "    if q1_s and q2_s:\n",
    "        return q1_s[-1] == q2_s[-1]\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "df_train['final_words'] = df_train.apply(lambda x: final_words(x['clean_q1'], x['clean_q2']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Length of questions\n",
    "def len_question(sentence):\n",
    "    return len(unigram(sentence))\n",
    "\n",
    "df_train['len_q1'] = df_train.apply(lambda x: len_question(x['clean_q1']), axis=1)\n",
    "df_train['len_q2'] = df_train.apply(lambda x: len_question(x['clean_q2']), axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Ratio of length between questions\n",
    "def len_ratio(q1,q2):\n",
    "    len_q1 = len_question(q1)\n",
    "    len_q2 = len_question(q2)\n",
    "    if len_q1 > len_q2:\n",
    "        return len_q2/len_q1\n",
    "    else:\n",
    "        if len_q2:\n",
    "            return len_q1/len_q2\n",
    "        else:\n",
    "            return len_q1\n",
    "\n",
    "len_ratio(q1,q2)\n",
    "df_train['len_ratio'] = df_train.apply(lambda x: len_ratio(x['clean_q1'], x['clean_q2']), axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Question frequency\n",
    "\n",
    "all_qid = df_train['qid1'].tolist() + df_train['qid2'].tolist()\n",
    "df = pd.DataFrame({'freq':all_qid})\n",
    "freq_serie = df['freq'].value_counts()\n",
    "total = len(all_qid)\n",
    "\n",
    "def freq_question(qid, total, serie):\n",
    "    nb_occ = serie.get(qid)\n",
    "    return nb_occ / total\n",
    "    \n",
    "df_train['freq_qid1'] = df_train.apply(lambda x: freq_question(x['qid1'],total,freq_serie), axis=1)\n",
    "df_train['freq_qid2'] = df_train.apply(lambda x: freq_question(x['qid2'],total,freq_serie), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_train.to_csv('../data/cleaned_train_with_ABCD_features.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# split data into X and y\n",
    "x_train = pd.DataFrame()\n",
    "x_train['jaccard_sim'] = df_train['jaccard_sim']\n",
    "x_train['shared_uni'] = df_train['shared_uni']\n",
    "x_train['shared_bi'] = df_train['shared_bi']\n",
    "x_train['shared_tri'] = df_train['shared_tri']\n",
    "x_train['norm_wmd'] = df_train['norm_wmd']\n",
    "x_train['final_words'] = df_train['final_words']\n",
    "x_train['len_q1'] = df_train['len_q1']\n",
    "x_train['len_q2'] = df_train['len_q2']\n",
    "x_train['len_ratio'] = df_train['len_ratio']\n",
    "x_train['freq_qid1'] = df_train['freq_qid1']\n",
    "x_train['freq_qid2'] = df_train['freq_qid2']\n",
    "x_train['tdidf'] = df_train['tdidf']\n",
    "\n",
    "y_train = df_train['is_duplicate'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient boosted decision trees\n",
    "\n",
    "XGBoost is used for supervised learning problems, where we use the training data (with multiple features) __X__ (here _x_train_ with features as columns) to predict a target variable __Y__ (here _y_train_ with 'is_duplicate')."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split data into train and test sets\n",
    "seed = 7\n",
    "test_size = 0.33\n",
    "\n",
    "x_train_set, x_test_set, y_train_set, y_test_set = train_test_split(\n",
    "    x_train, y_train, test_size=test_size, random_state=seed)\n",
    "\n",
    "# Model parameter\n",
    "params = {}\n",
    "params['objective'] = 'binary:logistic' # binary output\n",
    "params['eval_metric'] = 'logloss' # validation metrics\n",
    "params['eta'] = 0.04 # learning rate, default 0.03\n",
    "params['max_depth'] = 3 # Less over-fitting, default = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\teval-logloss:0.675273\ttrain-logloss:0.675298\n",
      "[50]\teval-logloss:0.397554\ttrain-logloss:0.397916\n",
      "[100]\teval-logloss:0.359662\ttrain-logloss:0.359589\n",
      "[150]\teval-logloss:0.346636\ttrain-logloss:0.346477\n",
      "[200]\teval-logloss:0.339821\ttrain-logloss:0.339515\n",
      "[250]\teval-logloss:0.335834\ttrain-logloss:0.335317\n",
      "[300]\teval-logloss:0.332596\ttrain-logloss:0.331884\n",
      "[350]\teval-logloss:0.329178\ttrain-logloss:0.328264\n",
      "[400]\teval-logloss:0.327192\ttrain-logloss:0.326092\n",
      "[450]\teval-logloss:0.325749\ttrain-logloss:0.324522\n",
      "[500]\teval-logloss:0.324194\ttrain-logloss:0.322834\n",
      "[550]\teval-logloss:0.323086\ttrain-logloss:0.321564\n",
      "[600]\teval-logloss:0.322239\ttrain-logloss:0.320513\n",
      "[650]\teval-logloss:0.321216\ttrain-logloss:0.319368\n",
      "[700]\teval-logloss:0.320488\ttrain-logloss:0.318508\n",
      "[750]\teval-logloss:0.319927\ttrain-logloss:0.317791\n",
      "[800]\teval-logloss:0.319121\ttrain-logloss:0.316908\n",
      "[850]\teval-logloss:0.318641\ttrain-logloss:0.316297\n",
      "[900]\teval-logloss:0.318176\ttrain-logloss:0.315706\n",
      "[950]\teval-logloss:0.317727\ttrain-logloss:0.315119\n",
      "[1000]\teval-logloss:0.317266\ttrain-logloss:0.314527\n",
      "[1050]\teval-logloss:0.316903\ttrain-logloss:0.314056\n",
      "[1100]\teval-logloss:0.316625\ttrain-logloss:0.313668\n",
      "[1150]\teval-logloss:0.31638\ttrain-logloss:0.313285\n",
      "[1200]\teval-logloss:0.316015\ttrain-logloss:0.312795\n",
      "[1250]\teval-logloss:0.315813\ttrain-logloss:0.312459\n",
      "[1300]\teval-logloss:0.315565\ttrain-logloss:0.312103\n",
      "[1350]\teval-logloss:0.315375\ttrain-logloss:0.311791\n",
      "[1400]\teval-logloss:0.3152\ttrain-logloss:0.311512\n",
      "[1450]\teval-logloss:0.315038\ttrain-logloss:0.311251\n",
      "[1500]\teval-logloss:0.314857\ttrain-logloss:0.310919\n",
      "[1550]\teval-logloss:0.314699\ttrain-logloss:0.310641\n",
      "[1600]\teval-logloss:0.314554\ttrain-logloss:0.310366\n",
      "[1650]\teval-logloss:0.31435\ttrain-logloss:0.310033\n",
      "[1700]\teval-logloss:0.314205\ttrain-logloss:0.309781\n",
      "[1750]\teval-logloss:0.314045\ttrain-logloss:0.309503\n",
      "[1800]\teval-logloss:0.313931\ttrain-logloss:0.309285\n",
      "[1850]\teval-logloss:0.313793\ttrain-logloss:0.309024\n",
      "[1900]\teval-logloss:0.313683\ttrain-logloss:0.308798\n",
      "[1950]\teval-logloss:0.313569\ttrain-logloss:0.308575\n",
      "[2000]\teval-logloss:0.313483\ttrain-logloss:0.30839\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "\n",
    "d_train = xgb.DMatrix(x_train_set, label=y_train_set)\n",
    "d_test = xgb.DMatrix(x_test_set, label=y_test_set)\n",
    "\n",
    "watchlist  = [(d_test,'eval'), (d_train,'train')]\n",
    "num_round = 2001\n",
    "bst = xgb.train(params, d_train, num_round, watchlist, verbose_eval=50)\n",
    "\n",
    "bst.save_model('../data/xgb_model.mdl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction on the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_test = pd.read_csv('../data/test.csv').fillna(\"\")\n",
    "\n",
    "# pre-process\n",
    "df_test['clean_q1'] = df_test.apply(lambda x: clean_question(x['question1'], stop_words), axis=1)\n",
    "df_test['clean_q2'] = df_test.apply(lambda x: clean_question(x['question2'], stop_words), axis=1)\n",
    "df_test.to_csv('../data/cleaned_test.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Features\n",
    "x_test = pd.DataFrame()\n",
    "x_test['jaccard_sim'] = df_test.apply(lambda x: jaccard(x['clean_q1'], x['clean_q2']), axis=1)\n",
    "x_test['shared_uni'] = df_test.apply(lambda x: shared_unigrams(x['clean_q1'], x['clean_q2']), axis=1)\n",
    "x_test['shared_bi'] = df_test.apply(lambda x: shared_bigrams(x['clean_q1'], x['clean_q2']), axis=1)\n",
    "x_test['shared_tri'] = df_test.apply(lambda x: shared_trigrams(x['clean_q1'], x['clean_q2']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_test['norm_wmd'] = df_test.apply(lambda x: wmd(x['clean_q1'], x['clean_q2'],word2vec_model), axis=1)\n",
    "x_test['final_words'] = df_test.apply(lambda x: final_words(x['clean_q1'], x['clean_q2']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_test['tdidf'] = df_test.apply(lambda x: tfidf(x['question1'],x['question2'],stop_words), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_test['len_q1'] = df_test.apply(lambda x: len_question(x['clean_q1']), axis=1)\n",
    "x_test['len_q2'] = df_test.apply(lambda x: len_question(x['clean_q2']), axis=1)\n",
    "x_test['len_ratio'] = df_test.apply(lambda x: len_ratio(x['clean_q1'], x['clean_q2']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "# Question frequency\n",
    "df_freq = pd.concat([df_train[['question1', 'question2']], \\\n",
    "        df_test[['question1', 'question2']]], axis=0).reset_index(drop='index')\n",
    "q_dict = defaultdict(set)\n",
    "for i in range(df_test.shape[0]):\n",
    "    q_dict[df_freq.question1[i]].add(df_freq.question2[i])\n",
    "    q_dict[df_freq.question2[i]].add(df_freq.question1[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def question_dic_freq(question):\n",
    "    return(len(q_dict[question]))\n",
    "        \n",
    "x_test['freq_qid1'] = df_test.apply(lambda x: question_dic_freq(x['question1']), axis=1)\n",
    "x_test['freq_qid2'] = df_test.apply(lambda x: question_dic_freq(x['question2']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_test.to_csv('../data/cleaned_test_with_ABCD_features.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_test2 = pd.read_csv('../data/cleaned_test_with_ABCD_features.csv').fillna(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_test = pd.DataFrame()\n",
    "x_test['jaccard_sim'] = df_test2['jaccard_sim']\n",
    "x_test['shared_uni'] = df_test2['shared_uni']\n",
    "x_test['shared_bi'] = df_test2['shared_bi']\n",
    "x_test['shared_tri'] = df_test2['shared_tri']\n",
    "x_test['norm_wmd'] = df_test2['norm_wmd']\n",
    "x_test['final_words'] = df_test2['final_words']\n",
    "x_test['len_q1'] = df_test2['len_q1']\n",
    "x_test['len_q2'] = df_test2['len_q2']\n",
    "x_test['len_ratio'] = df_test2['len_ratio']\n",
    "x_test['freq_qid1'] = df_test2['freq_qid1']\n",
    "x_test['freq_qid2'] = df_test2['freq_qid2']\n",
    "x_test['tdidf'] = df_test2['tdidf'].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bst = xgb.Booster(params)\n",
    "bst.load_model('../data/xgb_model.mdl')\n",
    "\n",
    "d_test = xgb.DMatrix(x_test)\n",
    "p_test = bst.predict(d_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "final = pd.DataFrame()\n",
    "final['test_id'] = df_test['test_id']\n",
    "final['is_duplicate'] = p_test\n",
    "final.to_csv('../data/output_xgb.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "final.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
